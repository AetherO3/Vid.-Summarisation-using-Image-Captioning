# -*- coding: utf-8 -*-
"""VidSumm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SAfnNG3B7f67EOLH2SNubuGGt9GkEI3
"""

!pip install kagglehub --quiet
!pip install torch torchvision --quiet
!pip install sentence-transformers --quiet
!pip install transformers --quiet
!pip install accelerate --quiet
!pip install safetensors --quiet
!pip install opencv-python --quiet
!pip install numpy --quiet

import os
import cv2
import glob
import torch
import kagglehub
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoProcessor, Blip2ForConditionalGeneration

path = kagglehub.dataset_download("vibhavvasudevan/avenue")

print(path)

video_dir = os.path.join(path, "avenue","testing", "frames", "03")
frames_path = os.path.join(video_dir, "*.jpg")
image_files = sorted(glob.glob(frames_path))

print(video_dir)

if not image_files:
    raise SystemExit("No frames found in folder 03.")

fps = 25
frame_numbers = [int(os.path.basename(f).split('.')[0]) for f in image_files]
timestamps = [fn / fps for fn in frame_numbers]

images = [cv2.imread(file) for file in image_files]
images = [img for img in images if img is not None]

if not images:
    print("Could not get the valid images")
    raise SystemExit

print(f"Loaded {len(images)} images successfully.")

device = "cuda" if torch.cuda.is_available() else "cpu"
processor = AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
dtype = torch.float16 if device == "cuda" else torch.float32

model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-opt-2.7b", torch_dtype=dtype).to(device)

captions = []

for i, img in enumerate(images):
    print(f"Image {i+1} of {len(images)} ({os.path.basename(image_files[i])})")

    inputs = processor(img, return_tensors="pt").to(device, dtype)
    generated_ids = model.generate(**inputs, max_new_tokens=20)

    generated_text = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True
    )[0].strip()

    print("Caption:", generated_text, "\n")
    captions.append(generated_text)

sumModel = SentenceTransformer('all-mpnet-base-v2')
embeds = sumModel.encode(captions, normalize_embeddings=True)

simScore = [np.dot(embeds[i], embeds[i + 1]) for i in range(len(embeds) - 1)]

print("Similarity scores:", simScore)

threshold = 0.30
clusters = []
current = [0]

for i, score in enumerate(simScore):
  if(score >= threshold):
    current.append(i + 1)
  else:
    clusters.append(current)
    current = [i + 1]

clusters.append(current)

print(clusters)

for idx, cluster in enumerate(clusters):
  start = cluster[0]
  end = cluster[-1]
  print(f"Between {timestamps[start]}s and {timestamps[end]}s")
  print(f"\t{captions[start]}")
  print(f"\n")
  print()

